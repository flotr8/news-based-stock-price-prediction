{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from helpFunctions import *\n",
    "\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "finbert_model = transformers.BertModel.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseBTCtime(d):\n",
    "    #Parse all Data on 30 minutes to aggregate on 60\n",
    "    if d.time().hour < 23:\n",
    "        d = d.replace(hour = d.time().hour + 1, minute = 0, second = 0)\n",
    "    else:\n",
    "        d = d+ ONE_DAY\n",
    "        d = d.replace(hour=0, minute=0)   \n",
    "    return d\n",
    "\n",
    "def encodeFinBERT(titles):\n",
    "    input_ids = finbert_tokenizer.encode(titles, return_tensors='pt', truncation=True, max_length=512, pad_to_max_length=True)\n",
    "    finbert_model.eval()\n",
    "    encoded_text = finbert_model(input_ids)[0]\n",
    "    sentence_embedding = encoded_text[0]\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "def configIndex(df, ticker, type):\n",
    "    if type == \"headlines\" and (ticker is None or ticker ==\"marketsNews\"):\n",
    "        df['Datetime'] = df['Date'] + \" \" + df['Time']\n",
    "        df.drop(columns = \"Unnamed: 0\", axis =0)\n",
    "    elif type == \"headlines\":\n",
    "        df['Datetime'] = df['Date']\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.set_index('Datetime')\n",
    "    return df\n",
    "\n",
    "def transformHeadlines1(df, s_d):\n",
    "    df = df.reset_index()\n",
    "    df['Datetime'] = df['Datetime'] - dt.timedelta(hours=6)\n",
    "    df['Datetime'] = df['Datetime'].apply(lambda row: parseBTCtime(row))\n",
    "    df.set_index('Datetime', inplace = True)\n",
    "    return df\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_symbols(text):\n",
    "    # Define regular expression pattern to match awkward symbols\n",
    "    symbol_pattern = r'[^\\w\\s\\.\\?\\!\\',;:\\-]'\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove symbols from text using regex\n",
    "    text = re.sub(symbol_pattern, '', text)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = 'ProsusAI/FinBERT'\n",
    "start = '2021-06-01'\n",
    "valid_start = '2022-02-01' \n",
    "fixed_valid = True                                   \n",
    "test_start = '2022-05-01'   \n",
    "end = '2023-04-30' \n",
    "re_train = False\n",
    "ticker = \"BTC-USD\"\n",
    "#Note: if cryptocurrency different parsing has to be configured (other stock exchange open times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Data\n",
    "h_general = configIndex(getLatestData(\"sentiments\", None, sentiment_model), None, \"sentiments\")\n",
    "h_market = configIndex(getLatestData(\"sentiments\", \"marketsNews\",sentiment_model), \"marketsNews\", \"sentiments\")\n",
    "h_stock = configIndex(getLatestData(\"sentiments\", ticker, sentiment_model), ticker, \"sentiments\")\n",
    "stock_data = configIndex(getLatestData(\"stock_data\", ticker, sentiment_model), ticker, \"stock_data\")\n",
    "\n",
    "#Filter on Relevant Data for Training & Prediction Range\n",
    "stock_data = stock_data[(stock_data.index > start) & (stock_data.index < end)]\n",
    "\n",
    "#Transform the Data\n",
    "h_gen = transformHeadlines1(h_general, stock_data)\n",
    "h_mar = transformHeadlines1(h_market, stock_data)\n",
    "h_stock = transformHeadlines1(h_stock, stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stock_data = stock_data[(stock_data.index > start) & (stock_data.index < end)]\n",
    "\n",
    "def mean_of_lists(lists):\n",
    "    # Concatenate the lists into a 2D array\n",
    "    arr = np.vstack(lists)\n",
    "    \n",
    "    # Take the mean along the first axis\n",
    "    result = np.mean(arr, axis=0)\n",
    "    \n",
    "    return list(result)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "length = 384\n",
    "#Sentences are encoded by calling model.encode()\n",
    "\n",
    "\n",
    "length = 384\n",
    "#Fill all Types of encodings in the list to be generated\n",
    "cols = [1, 10, 99]\n",
    "#1 = Last Headline\n",
    "#10 = Last 10 Headlines\n",
    "#99 = Mean of all Headlines\n",
    "\n",
    "\n",
    "news = ['GeneralNews', 'MarketNews', 'StockNews']\n",
    "#News Types to add\n",
    "\n",
    "for run in range (0,len(news)):\n",
    "    print(\"Run\")\n",
    "    print(run)\n",
    "    if run ==0:\n",
    "        headlines = [remove_symbols(str(headline)) for headline in h_gen[\"Headline\"].tolist()]\n",
    "        timestamps = h_gen.index.tolist()\n",
    "    if run == 1:\n",
    "        headlines = [remove_symbols(str(headline)) for headline in h_mar[\"Headline\"].tolist()]\n",
    "        timestamps = h_mar.index.tolist()\n",
    "    if run ==2:\n",
    "        headlines = [remove_symbols(str(headline)) for headline in h_stock[\"Headline\"].tolist()]\n",
    "        timestamps = h_stock.index.tolist()\n",
    "\n",
    "    embeddings = []\n",
    "    for headline in headlines:\n",
    "        embedding = model.encode(headline).tolist()\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    df = pd.DataFrame(index = timestamps)\n",
    "    df['Headline'] = embeddings\n",
    "    df.index = df.index.tz_localize('UTC')\n",
    "\n",
    "    for col in cols:\n",
    "        print(\"Column\")\n",
    "        print(col)\n",
    "        df_new = pd.DataFrame(columns = range(col), index = stock_data.index.unique() )\n",
    "        for timestamp in df_new.index:\n",
    "            subset = df[df.index == timestamp]\n",
    "            if col == 99:\n",
    "                if len(subset) >0:\n",
    "                    df_new.loc[timestamp,i] = mean_of_lists(subset['Headline'])\n",
    "                else:\n",
    "                    df_new.loc[timestamp, i] = [0] * length\n",
    "            else:\n",
    "                subset = subset.sort_index(ascending=False)[:col]\n",
    "                for i, (_, row) in enumerate(subset.iterrows()):\n",
    "                    df_new.loc[timestamp, i] = row['Headline']\n",
    "            \n",
    "                # Fill remaining columns with empty arrays\n",
    "                for i in range(len(subset), col):\n",
    "                    df_new.loc[timestamp, i] = [0] * length\n",
    "\n",
    "        prefix = ticker +\"_\" \n",
    "\n",
    "        merged_data = pd.merge(stock_data, df_new, how='left', left_index= True, right_index=True)\n",
    "\n",
    "        #write sentence embedding as pickle file to the drive\n",
    "        if col == 10:\n",
    "            merged_data = merged_data.loc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "            with open(prefix + str(run) + '_' + news[run] + '_Last' + str(col) + '_encoded.pkl', 'wb') as f:\n",
    "                pickle.dump(merged_data, f)\n",
    "        else:\n",
    "            merged_data = merged_data.loc[:, [0]]\n",
    "            if col == 99:\n",
    "                with open(prefix + str(run) + '_' + news[run] + '_Mean'  + '_encoded.pkl', 'wb') as f:\n",
    "                    pickle.dump(merged_data, f)\n",
    "            else:\n",
    "                with open(prefix + str(run) + '_' + news[run] + '_Last' +str(col) + '_encoded.pkl', 'wb') as f:\n",
    "                    pickle.dump(merged_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9902a808d1d8d5143c9550b3e19b072c613a6c44fd0a733e98a0e99f1b3b6d6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
