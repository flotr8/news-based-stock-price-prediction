{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helpFunctions import *\n",
    "from Headlines_Vectorizer import *\n",
    "import datetime as dt\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "from csv import writer\n",
    "from datetime import date\n",
    "from itertools import chain, combinations\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import talib\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from keras.models import *\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "                             TensorBoard)\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras import Input\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = list(range(5,116,10))\n",
    "windows = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50] \n",
    "ws_price = 10\n",
    "ws_news = 20\n",
    "windows = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "embedding_model = 'word2vec'\n",
    "runs = 1    \n",
    "start = '2021-04-09'\n",
    "valid_start = '2022-01-01' \n",
    "fixed_valid = True                                   \n",
    "test_start = '2022-04-01'   \n",
    "end = '2023-03-31' \n",
    "re_train = False\n",
    "ticker = \"AAPL\"\n",
    "n_past = 10\n",
    "sentiment_model =  'ProsusAI/finbert'\n",
    "tr_epochs = 20\n",
    "batch_size = 32\n",
    "model_type = \"LSTM with 3 Input Layers\"\n",
    "ONE_DAY = dt.timedelta(1)\n",
    "Classification = False\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "cols_news =['GeneralNews','MarketsNews','StockNews']\n",
    "cols_stock = ['Open', 'MiddleBand', 'UpperBand', 'LowerBand', 'Moving Average',\n",
    "              'Exp. Weighted MA', 'RSI', 'MoneyFlowIndex', 'FI']\n",
    "\n",
    "#cols_stock = ['Open']\n",
    "\n",
    "#cols_news =['GeneralNews', 'MarketsNews']\n",
    "cols_sentiment = ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore']    \n",
    "                             \n",
    "features = [['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore'], ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score'], ['GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore'], ['GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore'], ['GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore']]\n",
    "inputs = [cols_news, cols_stock, cols_sentiment]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Pre Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Help functions used for preprocessing and initalization of the dataset\n",
    "\n",
    "def generateOutputFile(name):\n",
    "    file_path = r'data/results/'                        # File Path for Results\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_path = file_path + 'results_' + name + '_' + timestr + '.csv'\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Generating File in \" + file_path)\n",
    "    print(\"-\" * 60)\n",
    "    header_row = ['Ticker','Features','n_past','Re-Train','RMSE','Accuracy','Actual Return abs', 'Actual Return %', 'Training Epochs', 'Batch Size','Start','End','Model Type', 'Train/Test']\n",
    "    with open(file_path, 'w') as res:\n",
    "        writer_object = writer(res)\n",
    "        writer_object.writerow(header_row)\n",
    "        res.close()\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def addDim(arr):\n",
    "    arr_reshaped = arr.reshape(arr.shape[0], arr.shape[1], arr.shape[2], 1)\n",
    "    return arr_reshaped\n",
    "\n",
    "def reshapeArray(arr2d):\n",
    "    # get the length of each list in the 2D array\n",
    "    list_length = len(arr2d[0][0])\n",
    "    # create a new 3D numpy array with shape (num_rows, num_cols, list_length)\n",
    "    new_shape = (arr2d.shape[0], arr2d.shape[1], list_length)\n",
    "    arr3d = np.empty(new_shape, dtype=object)\n",
    "    # loop over each element in the 2D array and assign it to the corresponding position in the 3D array\n",
    "    for i in range(arr2d.shape[0]):\n",
    "        for j in range(arr2d.shape[1]):\n",
    "            for k in range(list_length):\n",
    "                arr3d[i][j][k] = arr2d[j][i][k]\n",
    "\n",
    "    return arr3d\n",
    " \n",
    "def configIndex(df, ticker, type):\n",
    "    if type == \"headlines\" and (ticker is None or ticker ==\"marketsNews\"):\n",
    "        df['Datetime'] = df['Date'] + \" \" + df['Time']\n",
    "        df.drop(columns = \"Unnamed: 0\", axis =0)\n",
    "    elif type == \"headlines\":\n",
    "        df['Datetime'] = df['Date']\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
    "    df = df.set_index('Datetime')\n",
    "    return df\n",
    "\n",
    "def transformHeadlines(df, s_d):\n",
    "    df = df.reset_index()\n",
    "    df['Datetime'] = df['Datetime'] - dt.timedelta(hours=6)\n",
    "    df['Datetime'] = df['Datetime'].apply(lambda row: parseDatetime(row))\n",
    "    df['Counter'] = 1\n",
    "    df.loc[df['Positive'] >= df['Negative'], 'Label'] = 1\n",
    "    df.loc[df['Positive'] < df['Negative'], 'Label'] = -1\n",
    "    df.loc[df['Label'] == 1, 'PosNews'] = 1\n",
    "    df.loc[df['Label'] == -1, 'NegNews'] =1\n",
    "    df.drop_duplicates(subset = \"Headline\", keep =False, inplace = True)\n",
    "    df_grouped = df.groupby('Datetime').agg({'Counter': 'sum' , 'Positive':'sum',\n",
    "         'Negative': 'sum', 'Label': 'sum', 'PosNews': 'sum', 'NegNews':'sum', \n",
    "         'Headline': lambda x: ' '.join(x)})\n",
    "    df_grouped['length'] = df_grouped['Headline'].apply(lambda x: len(x))\n",
    "    df_grouped = pd.merge(s_d, df_grouped, 'left' , left_index = True, right_index = True)\n",
    "    df_grouped['Weighted_Score'] = df_grouped['Label'] / df_grouped['Counter']\n",
    "    df_grouped['Score'] = df_grouped['Positive'] - df_grouped['Negative']\n",
    "    df_grouped['Date'] = df_grouped.index.date\n",
    "    df_grouped['Daily_Score'] = df_grouped['Score'].groupby(df_grouped['Date']).cumsum()\n",
    "    df_grouped['Daily_Weighted_Score'] =df_grouped['Label'].groupby(df_grouped['Date']).cumsum()/ df_grouped['Counter'].groupby(df_grouped['Date']).cumsum() \n",
    "    return df_grouped\n",
    "\n",
    "def splitTrainTest(x_vector, x_prices, x_sentiments, y1, y2, stock_data, split_test, split_valid):\n",
    "    train_length = len(stock_data[stock_data.index < split_valid]) - n_past\n",
    "    valid_length = len(stock_data[stock_data.index < split_test]) - n_past\n",
    "    x_vector_train, x_prices_train, x_sentiments_train, y_train1, y_train2 = x_vector[:train_length], x_prices[:train_length], x_sentiments[:train_length], y1[:train_length], y2[:train_length]\n",
    "    x_vector_valid, x_prices_valid, x_sentiments_valid, y_valid1, y_valid2 = x_vector[train_length:valid_length], x_prices[train_length: valid_length], x_sentiments[train_length:valid_length],y1[train_length:valid_length],y2[train_length:valid_length]\n",
    "    valid_length = valid_length +1\n",
    "    x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2 = x_vector[valid_length:], x_prices[valid_length:], x_sentiments[valid_length:], y1[valid_length:], y2[valid_length:]\n",
    "    return x_vector_train, x_prices_train,x_sentiments_train, y_train1, y_train2, x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2, x_vector_valid, x_prices_valid, x_sentiments_valid, y_valid1, y_valid2\n",
    "\n",
    "def splitTrainTest_noValid(x_vector, x_prices, x_sentiments, y1, y2, stock_data, split_test, split_valid):\n",
    "    train_length = len(stock_data[stock_data.index < split_test]) - n_past\n",
    "    x_vector_train, x_prices_train, x_sentiments_train, y_train1, y_train2 = x_vector[:train_length], x_prices[:train_length], x_sentiments[:train_length], y1[:train_length], y2[:train_length]\n",
    "    x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2 = x_vector[train_length+1:], x_prices[train_length+1:], x_sentiments[train_length+1:], y1[train_length+1:], y2[train_length+1:]\n",
    "    return x_vector_train, x_prices_train,x_sentiments_train, y_train1, y_train2, x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2\n",
    "\n",
    "\n",
    "def createDatasets(df_complete, n_past):\n",
    "#generate subset for different inputs\n",
    "        x_price = []\n",
    "        x_sentiment = []\n",
    "        y1 = []\n",
    "        y2 = []\n",
    "\n",
    "        for i, row in df_complete.iterrows():\n",
    "            x_price.append(row[cols_stock])\n",
    "            x_sentiment.append(row[cols_sentiment])\n",
    "            y1.append(row['Actual Direction'])\n",
    "            #y2.append(row['Target'])\n",
    "            y2.append(row['Change'])\n",
    "\n",
    "\n",
    "        sc = MinMaxScaler()\n",
    "        sc_predict = MinMaxScaler()\n",
    "\n",
    "        x_price, x_sentiment, y1,y2 =  np.array(x_price), np.array(x_sentiment), np.array(y1), np.array(y2)\n",
    "        x_price, x_sentiment, y1,y2 =  x_price.astype(float), x_sentiment.astype(float), y1.astype(float), y2.astype(float)\n",
    "        x_price = sc.fit_transform(x_price)\n",
    "        x_sentiment = sc.fit_transform(x_sentiment)\n",
    "        y2 = y2.reshape(-1, 1)\n",
    "        y2 = sc_predict.fit_transform(y2)\n",
    "\n",
    "\n",
    "        #Create dataset incl. lookback\n",
    "        x_prices = []\n",
    "        x_sentiments = []\n",
    "        y1_ = []\n",
    "        y2_ = []\n",
    "\n",
    "        for i in range(n_past-1, len(y1)):\n",
    "            x_prices.append(x_price[i - n_past+1:i+1,: x_price.shape[1]])\n",
    "            x_sentiments.append(x_sentiment[i-n_past+1:i+1,: x_sentiment.shape[1]])\n",
    "            y1_.append(y1[i])\n",
    "            y2_.append(y2[i])\n",
    "\n",
    "        x_prices, x_sentiments, y1_, y2_= np.array(x_prices), np.array(x_sentiments), np.array(y1_),  np.array(y2_)\n",
    "\n",
    "        return x_prices, x_sentiments, y1_, y2_, sc, sc_predict\n",
    "\n",
    "\n",
    "def createlookback(data, lookback):\n",
    "    samples = data.shape[0] - lookback\n",
    "    features = data.shape[2]\n",
    "    lookback_data = np.zeros((samples+1, lookback, features))\n",
    "    for i in range(samples+1):\n",
    "        lookback_data[i] = data[i:i+lookback]\n",
    "    return lookback_data\n",
    "\n",
    "def lookback(data, lookback_window):\n",
    "    data_list = []\n",
    "    for i in range(len(data) - lookback_window+1):\n",
    "        window = data[i:(i + lookback_window)]\n",
    "        data_list.append(window)\n",
    "    return np.array(data_list).astype('float32')\n",
    "\n",
    "def create_input_arrays(df, ws_price, ws_news):\n",
    "    input_prices = df[cols_stock]\n",
    "    sentiment_scores = df[cols_sentiment]\n",
    "    labels = df['Actual Direction']\n",
    "    prices = df['Close']\n",
    "    input_prices_array = []\n",
    "    sentiment_scores_array = []\n",
    "    labels_array = []\n",
    "    prices_array = []\n",
    "    for i in range(len(df) - ws_price+1):\n",
    "        input_prices_array.append(input_prices.iloc[i:i+ws_price].values)\n",
    "        labels_array.append(labels.iloc[i+ws_price-1])\n",
    "        prices_array.append(prices.iloc[i+ws_price-1])\n",
    "\n",
    "    for i in range(len(df) - ws_news+1):\n",
    "        sentiment_scores_array.append(sentiment_scores.iloc[i:i+ws_news].values)\n",
    "\n",
    "\n",
    "    input_prices_array,sentiment_scores_array, labels_array, prices_array = np.array(input_prices_array), np.array(sentiment_scores_array), np.array(labels_array), np.array(prices_array)\n",
    "\n",
    "    prices_array =np.reshape(prices_array, (prices_array.shape[0], 1))\n",
    "    labels_array = np.reshape(labels_array,(labels_array.shape[0],1) )\n",
    "\n",
    "    sc_predict = MinMaxScaler()\n",
    "    prices_array = sc_predict.fit_transform(prices_array)\n",
    "\n",
    "    \n",
    "    return input_prices_array, sentiment_scores_array, labels_array, prices_array, sc_predict\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def flatTensorNumpy(encoded_data_general):\n",
    "    pad_size = 22\n",
    "    for i in range(len(encoded_data_general)):\n",
    "        if not isinstance(encoded_data_general[i], torch.FloatTensor):\n",
    "            encoded_data_general[i] = torch.zeros((pad_size, 768))\n",
    "\n",
    "        if len(encoded_data_general[i]) < pad_size:\n",
    "            encoded_data_general[i] =F.pad(encoded_data_general[i], (0, 0,0,pad_size -len(encoded_data_general[i])), value=0)\n",
    "\n",
    "\n",
    "    # Concatenate the tensors along a new dimension to flatten them\n",
    "    flattened_tensor = torch.cat([t.unsqueeze(0) for t in encoded_data_general], dim=0)\n",
    "    return flattened_tensor.numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelClass_incl_Embedding_directConcat(window_size, embedding_model):\n",
    "    \n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    sentiment_scores_input = Input(shape=(window_size, len(cols_sentiment)))\n",
    "    news_input = Input(shape=(window_size, 768, len(cols_news)))\n",
    "    \n",
    "    # reshape news_input\n",
    "    reshaped_news_input = Reshape((window_size, 768* len(cols_news)))(news_input)\n",
    "\n",
    "    # concatenate inputs\n",
    "    merged_input = Concatenate()([stock_prices_input, sentiment_scores_input, reshaped_news_input])\n",
    "\n",
    "    # LSTM models\n",
    "    lstm1 = LSTM(128, return_sequences=True)(merged_input)\n",
    "    lstm2 = LSTM(64, return_sequences=True)(lstm1)\n",
    "\n",
    "    # merge the output\n",
    "    merged = LSTM(64)(lstm2)\n",
    "\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(merged)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(merged)\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[stock_prices_input, sentiment_scores_input, news_input], outputs=[direction_output, price_output])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss={'direction_output': 'binary_crossentropy', 'price_output':  'mean_squared_error'},\n",
    "                  metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mean_squared_error']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_All(window_size, news_amount):\n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    embedding_input = Input(shape=(window_size, news_amount, 384, len(cols_news)))\n",
    "    sentiment_scores_input = Input(shape=(window_size,  len(cols_sentiment)))\n",
    "\n",
    "    filters = 8\n",
    "    \n",
    "\n",
    "    if news_amount == 1:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(news_amount, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, 1, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1,1))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, news_amount * filters))(conv_layer)\n",
    "    else:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(3, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, filters, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1, 2))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, filters * int(filters/2)))(pooling_layer)\n",
    "\n",
    "    #reshape_layer = Reshape((window_size, filters * int(filters/2)))(pooling_layer)\n",
    "\n",
    "\n",
    "    # LSTM models\n",
    "    stock_prices_lstm = LSTM(128, return_sequences= True)(stock_prices_input)\n",
    "    stock_prices_lstm = LSTM(64, return_sequences= True)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(32, return_sequences= False)(stock_prices_lstm)\n",
    "    stock_prices_lstm = Dropout(0.25)(stock_prices_lstm)\n",
    "\n",
    "    sentiment_scores_lstm = LSTM(128, return_sequences= True)(sentiment_scores_input)\n",
    "    sentiment_scores_lstm = LSTM(64, return_sequences= True)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = LSTM(32, return_sequences= False)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = Dropout(0.25)(sentiment_scores_lstm)\n",
    "\n",
    "    embedding_lstm = LSTM(256, return_sequences= True)(reshape_layer)\n",
    "    embedding_lstm = LSTM(128, return_sequences= True)(embedding_lstm)\n",
    "    embedding_lstm = LSTM(64, return_sequences= False)(embedding_lstm)\n",
    "    embedding_lstm = Dropout(0.25)(embedding_lstm)\n",
    "\n",
    "    # merge the output\n",
    "    merged = Concatenate()([embedding_lstm, sentiment_scores_lstm, stock_prices_lstm])\n",
    "    #merged = Dropout(0.25)(merged)\n",
    "\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(merged)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(merged)\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[embedding_input, sentiment_scores_input, stock_prices_input], outputs= [direction_output, price_output])\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', \n",
    "             loss={'direction_output': 'binary_crossentropy', 'price_output': ['mean_squared_error']},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "            metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mape']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_incl_Embedding(window_size, embedding_model):\n",
    "\n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    sentiment_scores_input = Input(shape=(window_size, len(cols_sentiment)))\n",
    "    news_input = Input(shape=(window_size, 768,3 ))\n",
    "    \n",
    "    # LSTM models\n",
    "    stock_prices_lstm = LSTM(128, return_sequences=True)(stock_prices_input)\n",
    "    stock_prices_lstm = LSTM(64, return_sequences=True)(stock_prices_lstm)\n",
    "    #stock_prices_lstm = Dropout(0.25)(stock_prices_lstm)\n",
    "    #p_output_stock = Dense(1, activation = 'linear')(stock_prices_lstm)\n",
    "    #d_output_stock = Dense(1, activation= 'sigmoid')(stock_prices_lstm)\n",
    "\n",
    "    sentiment_scores_lstm = LSTM(128, return_sequences=True)(sentiment_scores_input)\n",
    "    sentiment_scores_lstm = LSTM(64, return_sequences= True)(sentiment_scores_lstm)\n",
    "    #sentiment_scores_lstm = Dropout(0.25)(sentiment_scores_lstm)\n",
    "    #p_output_sent = Dense(1, activation = 'linear')(sentiment_scores_lstm)\n",
    "    #d_output_sent = Dense(1, activation='sigmoid')(sentiment_scores_lstm)\n",
    "\n",
    "    conv = Conv1D(filters=8, kernel_size=3, padding='same', activation='relu', data_format='channels_last')(news_input)\n",
    "    flat = Flatten()(conv)\n",
    "    reshaped = Reshape((-1, 128))(flat)\n",
    "    news_lstm = LSTM(128, return_sequences=True)(reshaped)\n",
    "    news_lstm = LSTM(64, return_sequences=True)(news_lstm)\n",
    "    #news_lstm = Dropout(0.25)(news_lstm)\n",
    "    #p_output_news = Dense(1, activation='linear')(news_lstm)\n",
    "    #d_output_news = Dense(1, activation='sigmoid')(news_lstm)\n",
    "\n",
    "\n",
    "    # merge the output\n",
    "    merged = Concatenate()([news_lstm, sentiment_scores_lstm,stock_prices_lstm])\n",
    "    merged = LSTM(64) (merged)\n",
    "\n",
    "    #merge the output\n",
    "\n",
    "    #price_output = layers.concatenate([p_output_stock, p_output_sent, p_output_news])\n",
    "    #direction_output = layers.concatenate([d_output_stock, d_output_sent, d_output_news])\n",
    "\n",
    "    #price_output = Dense(1, activation = 'linear',  name='price_output') (price_output)\n",
    "    #direction_output = Dense(1, activation = 'sigmoid', name='direction_output')(direction_output)\n",
    "\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(merged)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(merged)\n",
    "\n",
    "    \n",
    "    # create the model\n",
    "    model = Model(inputs=[news_input, sentiment_scores_input, stock_prices_input], outputs=[direction_output, price_output])\n",
    "\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', \n",
    "            loss={'direction_output': 'binary_crossentropy', 'price_output':  'mean_squared_error'},\n",
    "            loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "            metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mean_squared_error']})\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_no_Embedding(window_size):\n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    sentiment_scores_input = Input(shape=(window_size,  len(cols_sentiment)))\n",
    "    \n",
    "    # LSTM model\n",
    "    stock_prices_lstm = LSTM(128, return_sequences= True)(stock_prices_input)\n",
    "    stock_prices_lstm = LSTM(64, return_sequences= True)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(32, return_sequences= False)(stock_prices_lstm)\n",
    "    stock_prices_lstm = Dropout(0.2)(stock_prices_lstm)\n",
    "    sentiment_scores_lstm = LSTM(128, return_sequences= True)(sentiment_scores_input)\n",
    "    sentiment_scores_lstm = LSTM(64, return_sequences= True)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = LSTM(32, return_sequences= False)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = Dropout(0.2)(sentiment_scores_lstm)\n",
    "\n",
    "    # merge the output\n",
    "    merged = Concatenate()([sentiment_scores_lstm, stock_prices_lstm])\n",
    "    #merged = Dropout(0.5) (merged)\n",
    "\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(merged)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(merged)\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[sentiment_scores_input, stock_prices_input], outputs= [direction_output, price_output])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss={'direction_output': 'binary_crossentropy', 'price_output': ['mean_squared_error']},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "              metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mape']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_no_Embedding_directConcat(window_size):\n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    sentiment_scores_input = Input(shape=(window_size, len(cols_sentiment)))\n",
    "    \n",
    "    # input layers\n",
    "    combined_input = Concatenate(axis = 2)([stock_prices_input, sentiment_scores_input])\n",
    "\n",
    "    # LSTM model\n",
    "    lstm = LSTM(128, return_sequences=True)(combined_input)\n",
    "    #lstm = Dropout(0.2)(lstm)\n",
    "    lstm = LSTM(64, return_sequences = True)(lstm)\n",
    "    #lstm = Dropout(0.2)(lstm)\n",
    "    lstm = LSTM(32, return_sequences = False)(lstm)\n",
    "    lstm = Dropout(0.25)(lstm)\n",
    "\n",
    "    # output layers\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(lstm)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(lstm)\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[sentiment_scores_input,stock_prices_input], outputs= [direction_output, price_output])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss={\n",
    "               'direction_output': 'binary_crossentropy',\n",
    "                 'price_output': 'mape'\n",
    "               },\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "              metrics={\n",
    "        'direction_output': ['accuracy', 'binary_crossentropy'], \n",
    "        'price_output': 'mape'\n",
    "        })\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_onlySent(window_size):\n",
    "    # input layers\n",
    "    sentiment_scores_input = Input(shape=(window_size,  len(cols_sentiment)))\n",
    "    \n",
    "    # LSTM model\n",
    "    sentiment_scores_lstm = LSTM(128, return_sequences= True)(sentiment_scores_input)\n",
    "    sentiment_scores_lstm = LSTM(64, return_sequences= True)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = LSTM(32, return_sequences= False)(sentiment_scores_lstm)\n",
    "    sentiment_scores_lstm = Dropout(0.2)(sentiment_scores_lstm)\n",
    "\n",
    "\n",
    "\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(sentiment_scores_lstm) \n",
    "    price_output = Dense(1, activation='linear', name='price_output')(sentiment_scores_lstm )\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[sentiment_scores_input], outputs= [direction_output, price_output])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss={'direction_output': 'binary_crossentropy', 'price_output': ['mean_squared_error']},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "              metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mape']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_onlyEmbedd(window_size):\n",
    "    # input layers\n",
    "    embedding_input = Input(shape=(window_size, news_amount, 384, len(cols_news)))\n",
    "\n",
    "    filters = 8\n",
    "    if news_amount == 1:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(news_amount, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, 1, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1,1))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, news_amount * filters))(conv_layer)\n",
    "    else:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(3, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, filters, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1, 2))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, filters * int(filters/2)))(pooling_layer)\n",
    "\n",
    "\n",
    "    embedding_lstm = LSTM(256, return_sequences= True)(reshape_layer)\n",
    "    embedding_lstm = LSTM(128, return_sequences= True)(embedding_lstm)\n",
    "    embedding_lstm = LSTM(64, return_sequences= False)(embedding_lstm)\n",
    "    embedding_lstm = Dropout(0.5)(embedding_lstm)\n",
    "\n",
    "\n",
    "\n",
    "    # output layer)\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(embedding_lstm)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(embedding_lstm)\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[embedding_input], outputs= [direction_output, price_output])\n",
    "    model.compile(optimizer='adam', \n",
    "             loss={'direction_output': 'binary_crossentropy', 'price_output': ['mean_squared_error']},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "            metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mape']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_only_stock(window_size):\n",
    "    # input layer\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)), name= 'Stock Price Input')\n",
    "\n",
    "    # LSTM model\n",
    "    stock_prices_lstm = LSTM(128, return_sequences= True, name = 'LSTM_1')(stock_prices_input)\n",
    "    #stock_prices_lstm = Dropout(0.25)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(64, return_sequences = True, name = 'LSTM_2')(stock_prices_lstm)\n",
    "    #stock_prices_lstm = Dropout(0.25)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(32, name = 'LSTM_3')(stock_prices_lstm)\n",
    "    stock_prices_lstm = Dropout(0.25, name = 'Dropout')(stock_prices_lstm)\n",
    "    # output layer\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(stock_prices_lstm)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(stock_prices_lstm)\n",
    "\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=stock_prices_input, outputs=  [direction_output, price_output])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss={\n",
    "                'direction_output': 'binary_crossentropy',\n",
    "                 'price_output': 'mape'},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "              metrics={\n",
    "        'direction_output': ['accuracy', 'binary_crossentropy'], \n",
    "        'price_output': 'mape'})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelClass_Embedd_Price(window_size, news_amount):\n",
    "    # input layers\n",
    "    stock_prices_input = Input(shape=(window_size, len(cols_stock)))\n",
    "    embedding_input = Input(shape=(window_size, news_amount, 384, len(cols_news)))\n",
    "\n",
    "    filters = 8\n",
    "    \n",
    "\n",
    "    if news_amount == 1:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(news_amount, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, 1, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1,1))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, news_amount * filters))(conv_layer)\n",
    "    else:\n",
    "        conv_layer = Conv2D(filters=filters, kernel_size=(3, 384), activation='relu')(embedding_input)\n",
    "        reshape_layer = Reshape((window_size, filters, filters))(conv_layer)\n",
    "        pooling_layer = MaxPooling2D(pool_size=(1, 2))(reshape_layer)\n",
    "        reshape_layer = Reshape((window_size, filters * int(filters/2)))(pooling_layer)\n",
    "\n",
    "    #reshape_layer = Reshape((window_size, filters * int(filters/2)))(pooling_layer)\n",
    "\n",
    "\n",
    "    # LSTM models\n",
    "    stock_prices_lstm = LSTM(128, return_sequences= True)(stock_prices_input)\n",
    "    #stock_prices_lstm = Dropout(0.5)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(64, return_sequences= True)(stock_prices_lstm)\n",
    "    #stock_prices_lstm = Dropout(0.5)(stock_prices_lstm)\n",
    "    stock_prices_lstm = LSTM(32, return_sequences= False)(stock_prices_lstm)\n",
    "    stock_prices_lstm = Dropout(0.5)(stock_prices_lstm)\n",
    "    embedding_lstm = LSTM(256, return_sequences= True)(reshape_layer)\n",
    "    #embedding_lstm = Dropout(0.5)(embedding_lstm)\n",
    "    embedding_lstm = LSTM(128, return_sequences= True)(embedding_lstm)\n",
    "    #embedding_lstm = Dropout(0.5)(embedding_lstm)\n",
    "    embedding_lstm = LSTM(64, return_sequences= False)(embedding_lstm)\n",
    "    embedding_lstm = Dropout(0.5)(embedding_lstm)\n",
    "\n",
    "    # merge the output\n",
    "    merged = Concatenate()([embedding_lstm, stock_prices_lstm])\n",
    "    #merged = Dropout(0.25)(merged)\n",
    "\n",
    "    # output layer)\n",
    "    direction_output = Dense(1, activation='sigmoid', name='direction_output')(merged)\n",
    "    price_output = Dense(1, activation='linear', name='price_output')(merged)\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    model = Model(inputs=[stock_prices_input, embedding_input], outputs= [direction_output, price_output])\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', \n",
    "             loss={'direction_output': 'binary_crossentropy', 'price_output': ['mape']},\n",
    "              #loss_weights={'direction_output': 0.8, 'price_output': 0.2},\n",
    "            metrics={'direction_output': ['accuracy', 'binary_crossentropy'], 'price_output': ['mape']})\n",
    "\n",
    "    return model\n",
    "\n",
    "def modelAll(window_size, news_amount):\n",
    "    # input layers\n",
    "    price_input = Input(shape=(window_size, len(cols_stock)), name='price_input')\n",
    "    sentiment_input = Input(shape=(window_size, len(cols_sentiment)), name='sentiment_input')\n",
    "    news_input = Input(shape=(window_size, news_amount, 768, len(cols_news)), name='news_input')\n",
    "\n",
    "    # embedding layer for news input\n",
    "    news_embedding = TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu'))(news_input)\n",
    "    news_embedding = TimeDistributed(GlobalMaxPooling1D())(news_embedding)\n",
    "\n",
    "    # convolutional layer for sentiment input\n",
    "    sentiment_conv = Conv1D(filters=8, kernel_size=3, activation='relu')(sentiment_input)\n",
    "    sentiment_pool = GlobalMaxPooling1D()(sentiment_conv)\n",
    "\n",
    "    # concatenating all inputs\n",
    "    concat = Concatenate(axis=2)([price_input, sentiment_pool, news_embedding])\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm = LSTM(64, activation='relu', dropout=0.2, recurrent_dropout=0.2)(concat)\n",
    "\n",
    "    # output layers\n",
    "    price_output = Dense(1, name='price_output')(lstm)\n",
    "\n",
    "    # compile model\n",
    "    model = Model(inputs=[price_input, sentiment_input, news_input], outputs=price_output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evalModel(preds, data, evalType, pred_scaler):\n",
    "    result = data.copy()\n",
    "    result.loc[result['Close'] > result['Open'], 'Actual Direction'] = 1\n",
    "    result.loc[result['Close'] <= result['Open'], 'Actual Direction'] = 0\n",
    "    result['Actual_Change'] = result['Close'] - result['Open']\n",
    "    if not Classification:\n",
    "        result['Change'] = pred_scaler.inverse_transform(preds) \n",
    "        result.loc[result['Change'] > 0, 'Decision']  = 1\n",
    "        result.loc[result['Change'] <= 0, 'Decision']  = 0\n",
    "        result['Prediction'] = result['Change'] + result['Open']\n",
    "        #MSE = np.square(np.subtract(result['Prediction'], result['Close'])).mean() \n",
    "        MSE = np.square(np.subtract(result['Change'], result['Actual_Change'])).mean() \n",
    "        RMSE = math.sqrt(MSE)\n",
    "        #result.loc[result['Prediction'] > result['Open'], 'Decision'] = 1\n",
    "        #result.loc[result['Prediction'] <= result['Open'], 'Decision'] = 0\n",
    "        \n",
    "    else:\n",
    "        if len(preds[0]) > 1:\n",
    "            result['Prediction']= preds[0]\n",
    "            result['Change'] = pred_scaler.inverse_transform(preds[1])\n",
    "            result['Price_Prediction'] = result['Change'] + result['Open']\n",
    "            #result['Price_Prediction'] = preds[1]\n",
    "            MSE = np.square(np.subtract(result['Change'], result['Actual_Change'])).mean() \n",
    "            RMSE = math.sqrt(MSE)\n",
    "        else:\n",
    "            result['Prediction'] = preds\n",
    "            RMSE = 0\n",
    "\n",
    "        result.loc[result['Prediction'] > 0.5, 'Decision']  = 1\n",
    "        result.loc[result['Prediction'] <= 0.5, 'Decision']  = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    start_credit = 10000\n",
    "    credit = start_credit\n",
    "    # Return Calculation based on actual trades\n",
    "    for j in result.index:\n",
    "        amount = credit / result.loc[j, 'Open']              #replace credit with start_credit to ignore Zinseszins\n",
    "        #amount = 10                    \n",
    "        #if abs(result.loc[j, 'predicted'] - result.loc[j, 'open']) > 0.5: #Only trade \"Big Changes\"\n",
    "        if result.loc[j, 'Decision']:\n",
    "            result.loc[j, 'return'] = result.loc[j, 'Close'] * amount - result.loc[j, 'Open'] * amount\n",
    "        else:\n",
    "            result.loc[j, 'return'] = result.loc[j, 'Open'] * amount - result.loc[j, 'Close'] * amount\n",
    "        #else:\n",
    "         #   result.loc[j, 'return'] = 0\n",
    "        credit = credit + result.loc[j, 'return']\n",
    "        \n",
    "\n",
    "    return_abs = credit - start_credit\n",
    "    return_per = ((credit / start_credit) -1) *100\n",
    "\n",
    "    if len(preds[0]) > 1:\n",
    "        print('-' * 60)\n",
    "        print(\"Root Mean Square Error:\")\n",
    "        print(RMSE)\n",
    "        print('-' * 60)\n",
    "    print(\"\\nAccuracy: \")\n",
    "    print(accuracy_score(result['Actual Direction'], result['Decision']))\n",
    "    print(confusion_matrix(result['Actual Direction'], result['Decision']))\n",
    "    print('-' * 60)\n",
    "    print(\"\\nActual Return Absolut:\")\n",
    "    print(return_abs)\n",
    "    print('-' * 60)\n",
    "    print('-' * 60)\n",
    "    print(\"\\nActual Return %:\")\n",
    "    print(return_per)\n",
    "    print('-' * 60)\n",
    "\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    result.to_csv('data/results/' + evalType +  \"/Results_Evaluation_\" + emType + '_' +  str(n_past) + '_' + timestr +'.csv')\n",
    "    return RMSE, accuracy_score(result['Actual Direction'], result['Decision']), return_abs, return_per\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Execute PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Read in Data\n",
    "h_general = configIndex(getLatestData(\"sentiments\", None, sentiment_model), None, \"sentiments\")\n",
    "h_market = configIndex(getLatestData(\"sentiments\", \"marketsNews\",sentiment_model), \"marketsNews\", \"sentiments\")\n",
    "h_stock = configIndex(getLatestData(\"sentiments\", ticker, sentiment_model), ticker, \"sentiments\")\n",
    "stock_data = configIndex(getLatestData(\"stock_data\", ticker, sentiment_model), ticker, \"stock_data\")\n",
    "\n",
    "\n",
    "\n",
    "#create Technical Stock Indicators\n",
    "close = stock_data['Adj Close']\n",
    "stock_data['Moving Average'] = talib.SMA(close)\n",
    "stock_data['Exp Moving Average'] = talib.EMA(close)\n",
    "stock_data['MACD'] = talib.MACD(close )[1] #0 = Series, 1 = Signal, 2 = Divergence\n",
    "upper, middle, lower = talib.BBANDS(close)\n",
    "stock_data['UpperBBA'] = upper\n",
    "stock_data['MiddleBBA'] = middle\n",
    "stock_data['LowerBBA'] = lower\n",
    "stock_data['Momentum'] = talib.MOM(close)\n",
    "stock_data['Williams_R'] = talib.WILLR(stock_data['High'],stock_data['Low'], close,14)\n",
    "stock_data['RSI'] = talib.RSI(close)\n",
    "stock_data['STOCH_K'] = talib.STOCH(stock_data['High'],stock_data['Low'], close)[0]\n",
    "stock_data['STOCH_D'] = talib.STOCH(stock_data['High'],stock_data['Low'],close)[1]\n",
    "stock_data['ROCP'] = talib.ROCP(close)\n",
    "\n",
    "\n",
    "\n",
    "#Transform the Data\n",
    "h_gen = transformHeadlines(h_general, stock_data)\n",
    "h_mar = transformHeadlines(h_market, stock_data)\n",
    "h_stock = transformHeadlines(h_stock, stock_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in encoded Sentence Vectors\n",
    "\n",
    "with open('0_GeneralNews_Last1_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_general = pickle.load(f)\n",
    "    encoded_data_general = reshapeArray(encoded_data_general)\n",
    "\n",
    "\n",
    "with open('1_MarketNews_Last1_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_market = pickle.load(f)\n",
    "    encoded_data_market = reshapeArray(encoded_data_market)\n",
    "\n",
    "\n",
    "with open('2_StockNews_Last1_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_stock = pickle.load(f)\n",
    "    encoded_data_stock = reshapeArray(encoded_data_stock)\n",
    "\n",
    "\n",
    "with open('0_GeneralNews_Last10_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_general10 = pickle.load(f)\n",
    "    encoded_data_general10 = reshapeArray(encoded_data_general10)\n",
    "\n",
    "\n",
    "with open('1_MarketNews_Last10_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_market10 = pickle.load(f)\n",
    "    encoded_data_market10 = reshapeArray(encoded_data_market10)\n",
    "\n",
    "\n",
    "with open('2_StockNews_Last10_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_stock10 = pickle.load(f)\n",
    "    encoded_data_stock10 = reshapeArray(encoded_data_stock10)\n",
    "\n",
    "\n",
    "with open('0_GeneralNews_Mean_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_general_mean = pickle.load(f)\n",
    "    encoded_data_general_mean = reshapeArray(encoded_data_general_mean)\n",
    "\n",
    "\n",
    "with open('1_MarketNews_Mean_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_market_mean = pickle.load(f)\n",
    "    encoded_data_market_mean = reshapeArray(encoded_data_market_mean)\n",
    "\n",
    "\n",
    "with open('2_StockNews_Mean_encoded.pkl', 'rb') as f:\n",
    "    encoded_data_stock_mean = pickle.load(f)\n",
    "    encoded_data_stock_mean = reshapeArray(encoded_data_stock_mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct complete Dataframe\n",
    "d = {'GeneralNews': h_gen['Headline'], 'MarketsNews': h_mar['Headline'], 'StockNews' :h_stock['Headline'],\n",
    "        'GeneralNews_Score': h_gen['Score'], 'MarketsNews_Score': h_mar['Score'], 'StockNews_Score': h_stock['Score'],\n",
    "        'GeneralNews_WeightedScore': h_gen['Weighted_Score'], 'MarketsNews_WeightedScore': h_mar['Weighted_Score'], 'StockNews_WeightedScore': h_stock['Weighted_Score'],\n",
    "        'GeneralNews_DailyScore': h_gen['Daily_Score'], 'MarketsNews_DailyScore': h_mar['Daily_Score'], 'StockNews_DailyScore': h_stock['Daily_Score'],\n",
    "        'GeneralNews_Weighted_DailyScore': h_gen['Daily_Weighted_Score'], 'MarketsNews_Weighted_DailyScore': h_mar['Daily_Weighted_Score'], 'StockNews_Weighted_DailyScore': h_stock['Daily_Weighted_Score'],\n",
    "        'GeneralNewsCounter': h_gen['Counter'], 'MarketsNewsCounter': h_mar['Counter'], 'StockNewsCounter': h_stock['Counter'],\n",
    "        'GeneralNewsPosCounter': h_gen['PosNews'], 'MarketsNewsPosCounter': h_mar['PosNews'], 'StockNewsPosCounter': h_stock['PosNews'],\n",
    "        'GeneralNewsNegCounter': h_gen['NegNews'], 'MarketsNewsNegCounter': h_mar['NegNews'], 'StockNewsNegCounter': h_stock['NegNews'],\n",
    "        }\n",
    "h_all = pd.DataFrame(data = d)\n",
    "\n",
    "news_col =['GeneralNews','MarketsNews']\n",
    "\n",
    "#fill NaN for Text Data with \"None\", for numerical with 0\n",
    "for col in news_col:\n",
    "        h_all[col] = h_all[col].fillna(\"None\")\n",
    "h_all.fillna(0, inplace = True)\n",
    "df_complete = pd.merge(h_all, stock_data, \"inner\", left_index = True, right_index = True)\n",
    "df_complete.loc[df_complete['Close'] > df_complete['Open'], 'Actual Direction'] = 1\n",
    "df_complete.loc[df_complete['Close'] <= df_complete['Open'], 'Actual Direction'] = 0\n",
    "df_complete['Target'] = df_complete['Close']\n",
    "df_complete['Current_Open'] = df_complete['Open']\n",
    "df_complete['Change'] = df_complete['Close'] - df_complete['Open']\n",
    "df_complete['Actual_Change'] = df_complete['Change']\n",
    "df_complete['Hour'] = df_complete.index.hour\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parmeter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_stock = [\n",
    "              'Open', 'High', 'Low', 'Close', 'Adj Close', \n",
    "              'Volume', 'Moving Average',\n",
    "              'Exp Moving Average', 'MACD', 'UpperBBA', 'MiddleBBA', 'LowerBBA',\n",
    "              'Momentum', 'RSI', 'STOCH_K', 'STOCH_D', 'Williams_R', 'ROCP'\n",
    "       ]\n",
    "\n",
    "#Shift all due to no information leak by 1\n",
    "df_complete[cols_stock] = df_complete[cols_stock].shift(1)\n",
    "\n",
    "#Add current open price (no leak)\n",
    "cols_stock.extend(['Current_Open'])\n",
    "\n",
    "df_complete = df_complete[(df_complete.index > start) & (df_complete.index < end)]\n",
    "\n",
    "\n",
    "\n",
    "cols_news =['GeneralNews', 'MarketsNews', 'StockNews']\n",
    "cols_sentiment = ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore']  \n",
    "\n",
    "#cols_sentiment = ['GeneralNews_WeightedScore','MarketsNews_WeightedScore', 'StockNews_WeightedScore']  \n",
    "\n",
    "                            \n",
    "features = [['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore'], \n",
    "              ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score'], \n",
    "              ['GeneralNews_WeightedScore', 'MarketsNews_WeightedScore', 'StockNews_WeightedScore'], \n",
    "              ['GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore'], \n",
    "              ['GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore'], \n",
    "              ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore','MarketsNews_WeightedScore', 'StockNews_WeightedScore'],\n",
    "               ['GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore'],\n",
    "              ['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore'],\n",
    "              ['GeneralNews_WeightedScore','MarketsNews_WeightedScore', 'StockNews_WeightedScore','GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore']\n",
    "              ]\n",
    "\n",
    "features = [['GeneralNews_Score', 'MarketsNews_Score', 'StockNews_Score', 'GeneralNews_WeightedScore',\n",
    "              'MarketsNews_WeightedScore', 'StockNews_WeightedScore', 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore',\n",
    "              'GeneralNews_Weighted_DailyScore', 'MarketsNews_Weighted_DailyScore', 'StockNews_DailyScore'],\n",
    "              ['GeneralNews_Score', 'GeneralNews_WeightedScore', 'GeneralNews_DailyScore', 'GeneralNews_Weighted_DailyScore'],\n",
    "              ['MarketsNews_Score', 'MarketsNews_WeightedScore', 'MarketsNews_DailyScore', 'MarketsNews_Weighted_DailyScore'],\n",
    "              ['StockNews_Score', 'StockNews_WeightedScore', 'StockNews_DailyScore', 'StockNews_Weighted_DailyScore']\n",
    "              ] \n",
    "\n",
    "features = [[ 'GeneralNews_DailyScore', 'MarketsNews_DailyScore', 'StockNews_DailyScore'],\n",
    "              ['GeneralNews_DailyScore'],\n",
    "              [ 'MarketsNews_DailyScore'],\n",
    "              [ 'StockNews_DailyScore']\n",
    "              ] \n",
    "\n",
    "features = ['All', 'General', 'Market', 'Stock']\n",
    "inputs = [cols_news, cols_stock, cols_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of Experiments\n",
    "runs = 5     \n",
    "\n",
    "#Window Size\n",
    "windows = list(range(5,116,10))\n",
    "windows = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50] \n",
    "ws_price = 10\n",
    "ws_news = 20\n",
    "windows = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "windows = list(range(1,51,1))\n",
    "n_past = 41\n",
    "\n",
    "\n",
    "#Training Set Up\n",
    "tr_epochs = 50\n",
    "eps = [50]\n",
    "batch_size = 32\n",
    "#batches = [2,8,16,32,64]\n",
    "re_train = False\n",
    "model_type = \"LSTM with 3 Input Layers\"\n",
    "emType = 'L1'\n",
    "Classification = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Experiments and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_org = test_start\n",
    "valid_start_org = valid_start\n",
    "#Callbacks:\n",
    "es = EarlyStopping(monitor='val_direction_output_accuracy', min_delta=1e-10, patience=50, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_direction_output_accuracy', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_direction_output_accuracy', verbose=1, save_best_only=True, save_weights_only = False)\n",
    "tb = TensorBoard()\n",
    "\n",
    "\n",
    "\n",
    "#runs = len(windows)\n",
    "runs = 1\n",
    "for run in range(0,runs):\n",
    "    #n_past = windows[run]\n",
    "    #tr_epochs = eps[run]\n",
    "    #for i, window in enumerate(windows):\n",
    "    for i,feature in enumerate(features):\n",
    "        test_start = test_start_org\n",
    "        valid_start= valid_start_org\n",
    "        print(\"Start \" + start)\n",
    "        print(\"Valid Start \" + valid_start)\n",
    "        print(\"Test Start \" + test_start)\n",
    "        #n_past = window\n",
    "        \n",
    "        #cols.extend(list(feature))\n",
    "        #cols_sentiment = feature\n",
    "        print(\" \" * 60)\n",
    "        #print(\"Run: \" + str(i+1) + \"/\" +  str(len(windows)) + \" with Window = \" + str(window))\n",
    "        print(\"Run: \" + str(i+1) + \"/\" +  str(len(features)) + \" with Features = \" + str(cols_sentiment))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        x_prices, x_sentiments, y1, y2, sc, sc_predict = createDatasets(df_complete,n_past)\n",
    "\n",
    "        #x_vector = np.stack((encoded_data_general, encoded_data_market, encoded_data_stock),axis = 3)\n",
    "        news_amount = 10\n",
    "        if i <= 0:\n",
    "            cols_news =['GeneralNews', 'MarketsNews', 'StockNews']\n",
    "            x_vector = np.stack((encoded_data_general10, encoded_data_market10, encoded_data_stock10),axis = 3)\n",
    "            #x_vector = np.stack((encoded_data_general, encoded_data_market, encoded_data_stock),axis = 3)\n",
    "            #x_vector = np.stack((encoded_data_general_mean, encoded_data_market_mean, encoded_data_stock_mean),axis = 3)\n",
    "        elif i == 1:\n",
    "            cols_news =['GeneralNews']\n",
    "            x_vector = addDim(encoded_data_general10)\n",
    "            #x_vector = addDim(encoded_data_general)\n",
    "            #x_vector = addDim(encoded_data_general_mean)\n",
    "        elif i == 2:\n",
    "            cols_news =['MarketsNews']\n",
    "            x_vector = addDim(encoded_data_market10)\n",
    "            #x_vector = addDim(encoded_data_market)\n",
    "            #x_vector = addDim(encoded_data_market_mean)\n",
    "        else:\n",
    "            cols_news =['StockNews']\n",
    "            x_vector = addDim(encoded_data_stock10)\n",
    "            #x_vector = addDim(encoded_data_stock)\n",
    "            #x_vector = addDim(encoded_data_stock_mean)\n",
    "\n",
    "\n",
    "        \n",
    "        x_vector = lookback(x_vector, n_past)\n",
    "\n",
    "        stock_data = stock_data[(stock_data.index > start) & (stock_data.index < end)]\n",
    "        sd_filtered = stock_data.filter(['Open', 'Close'])\n",
    "        \n",
    "        x_vector_train, x_prices_train,x_sentiments_train, y_train1, y_train2, x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2, x_vector_valid, x_prices_valid, x_sentiments_valid, y_valid1, y_valid2 = splitTrainTest(x_vector, x_prices, x_sentiments, y1, y2, stock_data, test_start, valid_start)\n",
    "        #x_vector_train, x_prices_train,x_sentiments_train, y_train1, y_train2, x_vector_test, x_prices_test, x_sentiments_test, y_test1, y_test2 = splitTrainTest_noValid(x_vector, x_prices, x_sentiments, y1, y2, stock_data, test_start, valid_start)\n",
    "\n",
    "        y_train_org = y_train2.copy()\n",
    "        y_valid_org = y_valid2.copy()\n",
    " \n",
    "        \n",
    "        # Modelling \n",
    "        #x_vector = None\n",
    "        x_sentiments = None\n",
    "        prices = False\n",
    "\n",
    "\n",
    "        # Model ALL\n",
    "        if not x_vector is None and not x_prices is None and not x_sentiments is None:\n",
    "            if os.path.exists('weights.h5'):\n",
    "                os.remove('weights.h5')\n",
    "            \n",
    "            pred_model = modelClass_All(n_past, news_amount)\n",
    "            modelType = 'HybridOutput_incl_Embedding'\n",
    "            history = pred_model.fit(x = [x_vector_train, x_sentiments_train, x_prices_train], \n",
    "                                y = [y_train1, y_train2], \n",
    "                                epochs = tr_epochs, \n",
    "                                callbacks=[es, rlr, mcp, tb], \n",
    "                                validation_data = ((x_vector_valid, x_sentiments_valid, x_prices_valid), [y_valid1, y_valid2]),\n",
    "                                batch_size = batch_size)\n",
    "          \n",
    "\n",
    "            preds_train = pred_model.predict(x= [x_vector_train, x_sentiments_train, x_prices_train])\n",
    "            preds_valid =  pred_model.predict(x= [x_vector_valid, x_sentiments_valid, x_prices_valid])\n",
    "            preds_test = pred_model.predict(x= [x_vector_test, x_sentiments_test, x_prices_test])\n",
    "\n",
    "\n",
    "            if re_train:\n",
    "                data_test = stock_data[n_past + len(y_valid1) + len(y_train1):]\n",
    "                datelist_test = data_test.index\n",
    "                test_start = datelist_test[0].date()\n",
    "                prev_day = str(test_start)\n",
    "                rowCount = 0\n",
    "                preds1 = None\n",
    "                preds2 = None\n",
    "                for i,d in enumerate(datelist_test):\n",
    "                    if prev_day == str(d.date()):\n",
    "                        rowCount = rowCount + 1\n",
    "                    else:\n",
    "                        test_start = str(d.date())\n",
    "                        prev_day = str(d.date())\n",
    "                        valid_start = str((pd.to_datetime(valid_start) + ONE_DAY).date())\n",
    "                        new_preds = pred_model.predict(x = [x_vector_test[:rowCount],x_sentiments_test[:rowCount], x_prices_test[:rowCount]])\n",
    "                        if preds1 is None:\n",
    "                            preds1 = new_preds[0]\n",
    "                            preds2 = new_preds[1]\n",
    "                        else:\n",
    "                            preds1 = np.concatenate((preds1, new_preds[0]))\n",
    "                            preds2 = np.concatenate((preds2, new_preds[1]))\n",
    "\n",
    "\n",
    "                        rowCount = 1\n",
    "                        x_vector_train, x_prices_train,x_sentiments_train, y_train1, y_train2, x_vector_test, x_prices_test,x_sentiments_test, y_test1, y_test2, x_vector_valid, x_prices_valid, x_sentiments_valid, y_valid1, y_valid2 = splitTrainTest(x_vector, \n",
    "                                x_prices, x_sentiments, y1, y2, stock_data, test_start, valid_start)\n",
    "                        pred_model.fit(x = [x_vector_train, x_sentiments_train, x_prices_train], y = [y_train1, y_train2], \n",
    "                                epochs = 2,  callbacks=[es, rlr, mcp, tb], validation_data = ((x_vector_valid, x_sentiments_valid, x_prices_valid), (y_valid1, y_valid2)), batch_size = 32)\n",
    "                        #callbacks=[es, rlr, mcp, tb],\n",
    "\n",
    "                new_preds = pred_model.predict(x = [x_vector_test[:rowCount], x_sentiments_test[:rowCount], x_prices_test[:rowCount]])\n",
    "                preds1 = np.concatenate((preds1, new_preds[0]))\n",
    "                preds2 = np.concatenate((preds2, new_preds[1]))\n",
    "\n",
    "                preds_test_rt = [preds1, preds2]\n",
    "\n",
    "\n",
    "        # Model incl. Sentiments\n",
    "        elif prices and not x_sentiments is None:\n",
    "            if os.path.exists('weights.h5'):\n",
    "                os.remove('weights.h5')\n",
    "\n",
    "            pred_model = modelClass_no_Embedding(n_past)\n",
    "            modelType = 'HybridOutput_Sentiment_and_Price'\n",
    "            history = pred_model.fit(x = [x_sentiments_train, x_prices_train],\n",
    "                                y = [y_train1, y_train2] , epochs = tr_epochs, \n",
    "                                callbacks=[es, rlr, mcp, tb],\n",
    "                                validation_data = ((x_sentiments_valid, x_prices_valid), [y_valid1, y_valid2]), batch_size = batch_size)\n",
    "            \n",
    "            if os.path.exists('weights.h5'):\n",
    "                pred_model = load_model('weights.h5')\n",
    "\n",
    "            preds_train = pred_model.predict(x= [x_sentiments_train, x_prices_train])\n",
    "            preds_valid =  pred_model.predict(x= [x_sentiments_valid, x_prices_valid])\n",
    "            preds_test = pred_model.predict(x= [x_sentiments_test, x_prices_test])\n",
    "\n",
    "\n",
    "        #Model incl. Embeddings\n",
    "        elif prices and not x_vector is None:\n",
    "            pred_model = modelClass_Embedd_Price(n_past, news_amount)\n",
    "            modelType = 'HybridOutput_Prices_Embeddings_' + emType + ' ' + feature\n",
    "            history = pred_model.fit([x_prices_train, x_vector_train],   y = [y_train1,y_train2] ,epochs = tr_epochs, \n",
    "                                callbacks=[es, rlr, mcp, tb], \n",
    "                                validation_data = ((x_prices_valid, x_vector_valid), [y_valid1, y_valid2]), batch_size = batch_size)\n",
    "\n",
    "            preds_train = pred_model.predict(x= [x_prices_train, x_vector_train])\n",
    "            preds_valid =  pred_model.predict(x= [ x_prices_valid, x_vector_valid])\n",
    "            preds_test = pred_model.predict(x= [ x_prices_test, x_vector_test])\n",
    "        \n",
    "        #Model only Vectors\n",
    "        elif not x_vector is None and not prices and x_sentiments is None:\n",
    "            if os.path.exists('weights.h5'):\n",
    "                os.remove('weights.h5')\n",
    "\n",
    "            pred_model = modelClass_onlyEmbedd(n_past)\n",
    "            modelType = 'HybridOutput_just_Vectors'\n",
    "            history = pred_model.fit(x_vector_train,  [y_train1, y_train2] ,epochs = tr_epochs, \n",
    "                callbacks=[es, rlr, mcp, tb], \n",
    "                validation_data = (x_vector_valid, [y_valid1,y_valid2]), batch_size = batch_size)\n",
    "                        \n",
    "            if os.path.exists('weights.h5'):\n",
    "                pred_model = load_model('weights.h5')\n",
    "\n",
    "            preds_train = pred_model.predict(x= x_vector_train)\n",
    "            preds_valid =  pred_model.predict(x=  x_vector_valid)\n",
    "            preds_test = pred_model.predict(x=  x_vector_test)\n",
    "\n",
    "\n",
    "        #Model only Sentiments\n",
    "        elif not x_sentiments is None and not prices is None and x_vector is None:\n",
    "            if os.path.exists('weights.h5'):\n",
    "                os.remove('weights.h5')\n",
    "\n",
    "            pred_model = modelClass_onlySent(n_past)\n",
    "            modelType = 'HybridOutput_just_Sentiments'\n",
    "            history = pred_model.fit(x_sentiments_train,  [y_train1, y_train2] ,epochs = tr_epochs, \n",
    "                    callbacks=[es, rlr, mcp, tb], \n",
    "                    validation_data = (x_sentiments_valid, [y_valid1,y_valid2]), batch_size = batch_size)\n",
    "                        \n",
    "            if os.path.exists('weights.h5'):\n",
    "                pred_model = load_model('weights.h5')\n",
    "\n",
    "            preds_train = pred_model.predict(x= x_sentiments_train)\n",
    "            preds_valid =  pred_model.predict(x=  x_sentiments_valid)\n",
    "            preds_test = pred_model.predict(x=  x_sentiments_test)\n",
    "\n",
    "\n",
    "        #Modell only Stock\n",
    "        else:\n",
    "            if os.path.exists('weights.h5'):\n",
    "                os.remove('weights.h5')\n",
    "\n",
    "            pred_model = modelClass_only_stock(n_past)\n",
    "            modelType = 'HybridOutput_just_technical'\n",
    "            history = pred_model.fit(x_prices_train,  [y_train1, y_train2] ,epochs = tr_epochs, \n",
    "                    callbacks=[es, rlr, mcp, tb], \n",
    "                    validation_data = (x_prices_valid, [y_valid1,y_valid2]), batch_size = batch_size)\n",
    "            \n",
    "            if os.path.exists('weights.h5'):\n",
    "                pred_model = load_model('weights.h5')\n",
    "\n",
    "            preds_train = pred_model.predict(x= x_prices_train)\n",
    "            preds_valid =  pred_model.predict(x=  x_prices_valid)\n",
    "            preds_test = pred_model.predict(x=  x_prices_test)\n",
    "       \n",
    "\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        # Plot the loss values\n",
    "        plt.plot(train_loss, label='Training loss')\n",
    "        plt.plot(val_loss, label='Validation loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "        if x_vector is None:\n",
    "            cols_news = [\"\"]\n",
    "        cols_sent = cols_sentiment\n",
    "        if x_sentiments is None:\n",
    "            cols_sent = [\"\"]\n",
    "        if x_prices is None:\n",
    "            cols_stock = [\"\"]\n",
    "\n",
    "        if run == 0 and i == 0:\n",
    "            file_path =  generateOutputFile(modelType)\n",
    "            \n",
    "        print(\"#\" * 30)\n",
    "        print(\"Results Train Model:\")\n",
    "        RMSE_train, acc_train, act_return_train, acc_return_per_train = evalModel(preds_train, sd_filtered[n_past: n_past + len(y_train_org)], \"Training\", sc_predict)\n",
    "        print(\"#\" * 30)\n",
    "        print(\"Results Validation Model:\")\n",
    "        RMSE_valid, acc_valid, act_return_valid, acc_return_per_valid = evalModel(preds_valid, sd_filtered[n_past + len(y_train_org): n_past + len(y_valid_org) + len(y_train_org)], \"Validation\", sc_predict)\n",
    "        print(\"-\" * 30)\n",
    "        print(\"Results Test Model\")\n",
    "        RMSE_test, acc_test, act_return_test, acc_return_per_test = evalModel(preds_test, sd_filtered[n_past + len(y_train_org) + len(y_valid_org):], \"Test\", sc_predict)\n",
    "        \n",
    "        if re_train:\n",
    "            print(\"Results Re Trained Test Model\")\n",
    "            RMSE_test_rt, acc_test_rt, act_return_test_rt, acc_return_per_test_rt = evalModel(preds_test_rt, sd_filtered[n_past  + len(y_train_org) + len(y_valid_org):], \"Test\", sc_predict)\n",
    "\n",
    "        row_train= [ticker, cols_stock + cols_sent + cols_news, n_past, re_train, RMSE_train, acc_train, act_return_train, acc_return_per_train,  tr_epochs, batch_size, start, valid_start, model_type, 'Train']\n",
    "        row_test = [ticker, cols_stock + cols_sent +cols_news, n_past, re_train, RMSE_test, acc_test, act_return_test, acc_return_per_test,  tr_epochs, batch_size,test_start, end, model_type, 'Test']\n",
    "        if re_train:\n",
    "            row_test_rt = [ticker, cols_stock + cols_sent + cols_news, n_past, re_train, RMSE_test_rt, acc_test_rt, act_return_test_rt, acc_return_per_test_rt,  tr_epochs, batch_size, test_start, end, model_type, 'Test Re Trained']\n",
    "        \n",
    "        with open(file_path, 'a') as res:\n",
    "            writer_object = writer(res)\n",
    "            writer_object.writerow(row_train)\n",
    "            writer_object.writerow(row_test)\n",
    "            if re_train:\n",
    "                writer_object.writerow(row_test_rt)\n",
    "            res.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model and save to file\n",
    "from IPython.display import SVG\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "plot_model(pred_model, to_file='data/thesis/modelStock.png')\n",
    "\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_complete[(df_complete.index > start) & (df_complete.index < valid_start)]\n",
    "df_valid = df_complete[(df_complete.index >= valid_start) & (df_complete.index < test_start)]\n",
    "df_test = df_complete[(df_complete.index >= test_start) & (df_complete.index < end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_csv('datasetcomplete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['GeneralNewsCounter'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, valid_start, test_start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['MarketsNewsPosCounter'].sum(), res['MarketsNewsNegCounter'].sum(), res['MarketsNewsCounter'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9902a808d1d8d5143c9550b3e19b072c613a6c44fd0a733e98a0e99f1b3b6d6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
